{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 統計學習與深度學習\n",
    "### Homework 2\n",
    "\n",
    "\n",
    "請將IPYNB檔與IPYNB Export之HTML檔上傳至COOL作業區。回答作業時建議使用 \"三明治\" 答題法。也就是說，先說明要做什麼，然後列出程式碼與結果，最後說明這些結果的意義。作業自己做。嚴禁抄襲。不接受紙本繳交，不接受遲交。請以英文或中文作答。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一題 [Data Preprocessing]\n",
    "\n",
    "(10%) 資料前處理是一個重要的工作，本題將利用UCI的\"Adult\" dataset <https://archive.ics.uci.edu/ml/datasets/Adult>來練習資料前處理。我們使用這個資料集的方式是用來建構預測最後一個收入欄位是'>50K'或'<=50K'。這個資料集已經先切好了Training跟Test。我們將會沿用這個切割。\n",
    "\n",
    "資料前處理包含以下工作:\n",
    "* 生成以下numpy變數: x_train(訓練特徵)、y_train(訓練標籤)、x_test(測試特徵)、y_test(測試標籤)。用一個Dictionary組織將這些變數，其中Key為變數名稱，Value為之前生成的變數內容。\n",
    "* 最後一欄為標籤，將'>50K'與'<=50K'轉成1跟0。其他欄位為特徵。\n",
    "* 把所有含有缺值的Rows刪除。\n",
    "* 所有數值欄位標準化(均數為0，變異數為1)。測試資料特徵需用訓練資料的均數與變異數標準化。\n",
    "* 所有類別欄位(如native-country與workclass)都應使用\"1-of-K\"轉換成0與1的欄位。\n",
    "* 我們只考慮在訓練資料中出現超過(含)10次的特徵值。如果一個特徵值出現不到10次，則刪除這個特徵值所對應的1-of-K欄位。\n",
    "* 你可以使用sklearn中的工具函數進行1-of-K encoding與變數標準化。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前處理完成後，比較你生成的Dictionary與由**adult_m50k.pickle**讀入的資料比較，確定內容相同。\n",
    "\n",
    "讀取**adult_m50k.pickle**的方式如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個adult50kp是個Dictionary，裡面有:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_train', 'y_train', 'x_test', 'y_test', 'columnname', 'num_col'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult50kp.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中'x_train', 'y_train', 'x_test', 'y_test'分別是訓練資料特徵、訓練資料標記、測試資料特徵、測試資料標記。'num_col'是連續變數特徵的欄位名稱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['capital-loss',\n",
       " 'hours-per-week',\n",
       " 'capital-gain',\n",
       " 'educational-num',\n",
       " 'age',\n",
       " 'fnlwgt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult50kp['num_col']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'columnname'是所有特徵資料的欄位名稱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['capital-loss', 'hours-per-week', 'capital-gain',\n",
       "       'educational-num', 'age', 'fnlwgt', 'relationship_Husband',\n",
       "       'relationship_Not-in-family', 'relationship_Other-relative',\n",
       "       'relationship_Own-child', 'relationship_Unmarried',\n",
       "       'relationship_Wife', 'race_Amer-Indian-Eskimo',\n",
       "       'race_Asian-Pac-Islander', 'race_Black', 'race_Other',\n",
       "       'race_White', 'gender_Female', 'gender_Male',\n",
       "       'occupation_Adm-clerical', 'occupation_Craft-repair',\n",
       "       'occupation_Exec-managerial', 'occupation_Farming-fishing',\n",
       "       'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct',\n",
       "       'occupation_Other-service', 'occupation_Priv-house-serv',\n",
       "       'occupation_Prof-specialty', 'occupation_Protective-serv',\n",
       "       'occupation_Sales', 'occupation_Tech-support',\n",
       "       'occupation_Transport-moving', 'education_10th', 'education_11th',\n",
       "       'education_12th', 'education_1st-4th', 'education_5th-6th',\n",
       "       'education_7th-8th', 'education_9th', 'education_Assoc-acdm',\n",
       "       'education_Assoc-voc', 'education_Bachelors',\n",
       "       'education_Doctorate', 'education_HS-grad', 'education_Masters',\n",
       "       'education_Preschool', 'education_Prof-school',\n",
       "       'education_Some-college', 'native-country_Cambodia',\n",
       "       'native-country_Canada', 'native-country_China',\n",
       "       'native-country_Columbia', 'native-country_Cuba',\n",
       "       'native-country_Dominican-Republic', 'native-country_Ecuador',\n",
       "       'native-country_El-Salvador', 'native-country_England',\n",
       "       'native-country_France', 'native-country_Germany',\n",
       "       'native-country_Greece', 'native-country_Guatemala',\n",
       "       'native-country_Haiti', 'native-country_Honduras',\n",
       "       'native-country_Hong', 'native-country_Hungary',\n",
       "       'native-country_India', 'native-country_Iran',\n",
       "       'native-country_Ireland', 'native-country_Italy',\n",
       "       'native-country_Jamaica', 'native-country_Japan',\n",
       "       'native-country_Laos', 'native-country_Mexico',\n",
       "       'native-country_Nicaragua',\n",
       "       'native-country_Outlying-US(Guam-USVI-etc)', 'native-country_Peru',\n",
       "       'native-country_Philippines', 'native-country_Poland',\n",
       "       'native-country_Portugal', 'native-country_Puerto-Rico',\n",
       "       'native-country_Scotland', 'native-country_South',\n",
       "       'native-country_Taiwan', 'native-country_Thailand',\n",
       "       'native-country_Trinadad&Tobago', 'native-country_United-States',\n",
       "       'native-country_Vietnam', 'native-country_Yugoslavia',\n",
       "       'workclass_Federal-gov', 'workclass_Local-gov',\n",
       "       'workclass_Private', 'workclass_Self-emp-inc',\n",
       "       'workclass_Self-emp-not-inc', 'workclass_State-gov',\n",
       "       'workclass_Without-pay', 'marital-status_Divorced',\n",
       "       'marital-status_Married-AF-spouse',\n",
       "       'marital-status_Married-civ-spouse',\n",
       "       'marital-status_Married-spouse-absent',\n",
       "       'marital-status_Never-married', 'marital-status_Separated',\n",
       "       'marital-status_Widowed'], dtype='<U41')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult50kp['columnname']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了能方便的比較，你所生成的Dictionary中x_train與x_test的欄位順序應該與adult50kp['columnname']相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "假設你生成的Dictionary叫adult50k，下面的範例程式比較這個變數與由picke檔案讀入的adult50kp中四個主要變數是否相同:\n",
    "\n",
    "```python\n",
    "elems = ['x_train', 'x_test', 'y_train', 'y_test']\n",
    "\n",
    "for aelem in elems:\n",
    "    cnomatch = np.sum(adult50kp[aelem] != adult50k[aelem])\n",
    "    if cnomatch == 0:\n",
    "        print(aelem, \"match!\")\n",
    "    else:\n",
    "        print(aelem, \"%d elements no match!\" % cnomatch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了讓大家更清楚欄位的安排，以下針對位在17與18欄(Index由0開始)的'gender_Female', 'gender_Male'操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gender_Female', 'gender_Male'], dtype='<U41')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colname = adult50kp['columnname']\n",
    "colname[17:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們取出這兩欄來看看:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = adult50kp['x_train']\n",
    "x_train[:, 17:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這兩欄應該只有其中一個是1，另一個是0。因此By Rows加總會是1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:, 17:19].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二題 [ROC and AUC]\n",
    "(35%) Receiver operation characteristic (ROC)曲線以及其線下面積 (Area Under Curve; AUC)為衡量分類器預測能力常用的工具。本題將練習繪製ROC以及計算AUC。\n",
    "在這之前我們必須載入資料，訓練模型，並進行預測:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.848340\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# load dataset\n",
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "    \n",
    "# train prediction model    \n",
    "c = 0.3\n",
    "lr2 = LogisticRegression(solver = 'lbfgs', C= c, max_iter = 1000)\n",
    "lr2.fit(adult50kp['x_train'], adult50kp['y_train'])\n",
    "# make prediction\n",
    "ypred = lr2.predict(adult50kp['x_test'])\n",
    "ypredprob = lr2.predict_proba(adult50kp['x_test'])\n",
    "# compute accuracy\n",
    "ncorrect = np.sum(adult50kp['y_test'] == ypred)\n",
    "accuracy_sk = ncorrect / adult50kp['y_test'].shape[0]\n",
    "print(\"Accuracy = %f\" % accuracy_sk)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回答下面問題:\n",
    "\n",
    "* Q2.1 (17.5%): 基於`adult50kp['y_test']`與`ypredprob`繪製ROC Curve。\n",
    "* Q2.2 (17.5%): 計算繪製出的ROC Curve的AUC。\n",
    "\n",
    "規定與提示:\n",
    "* 禁用現成的ROC與AUC計算函數，如 `sklearn.metrics.roc_curve`與`sklearn.metrics.auc`。違者本題零分。\n",
    "* 計算AUC時可以利用相鄰的FP_Rate與其對應的TP_Rate所形成的梯形近似該小塊面積，然後加總所有梯形的面積即可得到AUC。梯形面積計算請參考下圖。\n",
    "\n",
    "![AUC Tip](tipsauc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三題 [Logistic Regression with L2 Regularization]\n",
    "\n",
    "(55%) The Logistic regression with L2 regularization minimize the following error function:\n",
    "\n",
    "\n",
    "$\\frac{\\lambda}{2} w^T w - \\sum_{i=1}^n [ t_i \\ln y_i  + (1 - t_i) \\ln (1 - y_i)],$\n",
    "\n",
    "where $y_i = \\frac{1}{1 + exp({-w^T x_i})}$ and $t_i \\in \\{0, 1\\}$ is the label value, $x_i$ is the feature vector, and $w$ is the regression coefficient vector. \n",
    "\n",
    "\n",
    "We are going to consider an extension of this model to allow different levels of regularization for different regression coefficients. Consider the constant term versus other features. The coefficient of the constant term is usually not regularized in logistic regression. It is because the constant term is related to the odds ratio when all features are zero, and regularizing this term will force the probability of the positive class given a zero feature vector to be 0.5, which may or may not be reasonable.  \n",
    "\n",
    "Another consideration is regarding the continuous-valued features and binary-valued features. We typically normalize continuous-valued features to have zero means and unit variances but keep binary-value features untouched. It makes sense to have a single regularization value for the continuous-valued features since all of them have been normalized. Similarly, if we do not have additional information, then all binary-valued features can have the same level of regularization. However, using the same regularization coefficient for the continuous-valued and binary-valued features may not be reasonable. That is, it is often beneficial to have a regularization coefficient for the continuous-valued features and another regularization coefficient for the binary-valued features. \n",
    "\n",
    "The above discussion suggests that a more sophisticated way to regularize a logistic regression is to have three regularization coefficients: 0 for the constant, $a_1$ for continuous-valued features, and $a_2$ for the binary-valued features. It is possible to further refine the regularization coefficients. However, hyper-parameter tuning associated with more regularization coefficients may be costly. \n",
    "\n",
    "To achieve this goal, we are going to consider a variation of L2-regularized logistic regression that allows different levels of regularization for each coefficient. In the following discussion, we are going to use $X$ to denote the feature matrix in the training data. The i-th row in $X$, $x_i$, is the feature vector for the i-th training data. The last column of $X$ is one unless we do not include the constant term. \n",
    "\n",
    "In this model, each regression coefficient may be associated with a different regularization coefficient. Bearing with the risk of ambigulity, we (again) use the scalar $\\lambda_k$ to denote the regularization coefficient for $w_k$.  The vector $w = [w_1, w_2, ..., w_D]^T$ is the  regression coefficient vector. Let $\\Lambda$ denote the diagonal matrix that have $\\lambda_k$ at $\\Lambda_{kk}$. Our new error function becomes: \n",
    "\n",
    "$E(w) = \\frac{1}{2} w^T \\Lambda w - \\sum_{i=1}^n [ t_i \\ln y_i  + (1 - t_i) \\ln (1 - y_i)],$\n",
    "\n",
    "where $y_i = \\frac{1}{1 + exp({-w^T x_i})}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model allows $w_k$ to have regularization coefficient $\\lambda_k$. If the constant term is the last element in $w$, then setting $\\lambda_D$ to $0$ allows us to free the constant term from regularization. We can set $\\lambda_k$ associated with continuous-valued features to one value, and elements associated with binary-value features to another value. This will achieve our goal of a more refined regularization structure. \n",
    "\n",
    "Following the PRML textbook and the class discussion, we are going to train the model using the Newton-Raphson optimization method. In order to do so, you need to derive the gradient and hessian of $E(w)$. Given the training dataset, we can optimize $w$ via \n",
    "\n",
    "$w^{(new)} = w^{(old)} - H^{-1} \\nabla E$\n",
    "\n",
    "To do so, we need to have an initial vector of $w$ to kick start the iteration. One way to do this is to use the closed-form solution of ridge regression: $w = (X^T X + b I)^{-1} X^T t$, where $t$ is the vector of training labels. Set $b$ to the average of $\\lambda_i$. Another way is to change the original L2 regularization term in ridge regression to $\\frac{1}{2}w^T \\Lambda w$ and derive the new closed-form solution that matches our model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Python class named mylogistic_l2 that performs model training and prediction. \n",
    "\n",
    "The sample usage should be like the following:\n",
    "```python\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic1.fit(X_train, Y_train)\n",
    "ypred = logic1.predict(X_test)\n",
    "```\n",
    "The first line is to create an object with the specified regularization coefficient vector, lambda_vec, and set the maximum number of iteration to 1000. The \"tol\" parameter sets the stopping condition for Newton-Raphson optimization. The iteration will stop if the improvement on the error function is less than $10^{-5}$. The \"add_intercept\" option says that we need to add a column of ones to the end of X_train before model training. The length of lambda_vec, as a result, should match the number of columns after adding the \"one column\" when this option is turned on. \n",
    "\n",
    "Use the following skeleton to create your mylogistic_l2 class:\n",
    "```python\n",
    "class mylogistic_l2():\n",
    "    def __init__(self, reg_vec, max_iter = 100, tol = 1e-5, add_intercept = True):\n",
    "        \"\"\"reg_vec: the regularization coefficient vector\n",
    "           max_iter: maximum number of iteration to run for the Newton method\n",
    "           tol: tolerance for the objective function\n",
    "           add_intercept: whether to add intercept (a column of ones) at last column of the feature matrix\"\"\"\n",
    "        ### Add your code here\n",
    "\n",
    "    def fit(self, x, y, verbal = False):\n",
    "        # Add your code here\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"doing prediction\"\"\"\n",
    "        ### add your code here.     \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "To simplify the discussion, we use 0.5 as the threshold for the positive case when making predictions. That is, the output of the last line should be a numpy array of 0 and 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Restrictions\n",
    "You are allowed to use the \"building block\" libraries including numpy and scipy in your own mylogistic_l2 class. You will receive a zero score if you adopted an existing logistic regression classifier in your answer. The input features and labels for the training method should be numpy arrays. The input features and output labels for the predict method should be numpy arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "We are going to use to \"Adult\" dataset on the UCI machine learning reposition <https://archive.ics.uci.edu/ml/datasets/Adult>. Do not download the raw data from the website. Instead, load the processed data from **adult_m50k.pickle** using the following sample code: \n",
    "\n",
    "```python\n",
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "```    \n",
    "    \n",
    "You can access the training and test data using the following keys: 'x_train', 'x_test', 'y_train', 'y_test'. In addition, the key 'columnname' map the a list of column names, and the key 'num_col' map to a list of numeric columns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "* Q3.1 (15%) Derive the gradient and hessian matrix for the new E(w). \n",
    "* Q3.2 (25%) Create your mylogistic_l2 class. Train your model and show the learned $w$ as well as test accuracy for the cases below. If $w$ is too long for you, show selected $w$ for continuous-valued, binary-valued, and the constant term.  \n",
    "    * Case 1: lambda = 1 for all coefficients\n",
    "    * Case 2: lambda = 1 for all but the intercept, no regularization for intercept term.\n",
    "    * Case 3: lambda = 1 for numerical-valued features, lambda = 0.5 for binary-valued features, no regularization for intercept term.\n",
    "* Q3.3 (10%) Further split the training data into subtraining (90%) and tuning (10%) to search for the best hyperparameters. Set the regularization coefficient for the constant term to zero. Allow different regularizations for continuous-valued and binary-valued features. Let $a_1$ and $a_2$ denote the regularization coefficients for continuous-valued and binary-valued features. Search the best $a_1$ and $a_2$ and report the test accuracy using the best hyper-parameters. You should follow the following procedure to search for the best hyperparameters. \n",
    "    1. Choose a set of grids among a reasonable range. For example, 10 grids in [0.01, 100]. \n",
    "    2. Conduct grid search with the constraint that $a_1 = a_2$. Record the best value $a_1^*$ and $a_2^*$.\n",
    "    3. Fix $a_1 = a_1^*$, and search $a_2$ for the best value, call the result the new $a_2^*$. \n",
    "    4. Fix $a_2 = a_2^*$, and search $a_1$ for the best value.\n",
    "    5. Report the selected $a_1$ and $a_2$.\n",
    "    6. Train a model using the selected hyper-parameters, and report the test accuracy. \n",
    "  \n",
    "* Q3.4 (5%) Use sklearn.linear_model.LogisticRegression to train and test the model (including hyperparameter tuning). Compare the estimated parameters and test accuracy with those from your own models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
